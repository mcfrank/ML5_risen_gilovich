---
title: ''
author: ''
header-includes:
- \usepackage[T1]{fontenc}
- \usepackage{microtype}
- \usepackage[margin=1in]{geometry}
- \usepackage{fancyhdr}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead{}
- \fancyfoot{}
- \fancyhead[C]{ML5: Risen \& Gilovich}
- \fancyfoot[RO,LE]{\thepage}
- \usepackage{titlesec}
- \usepackage{booktabs}
- \usepackage{lettrine}
- \usepackage{paralist}
- \usepackage{setspace}\doublespacing
- \usepackage{natbib}
- \setcitestyle{apalike}
- \usepackage{url}
- \usepackage{parskip}
- \usepackage{color,soul}
output:
  pdf_document:
    citation_package: natbib
  word_document: default
bibliography: refs_ml5.bib
---

\doublespacing

\begin{center}
\textbf{ \LARGE{Challenges in replicating Risen \& Gilovich (2008):} } \\ \vspace{5mm}
\textbf{ \LARGE{a registered multisite replication} }
\vspace{10mm}
\end{center}

\doublespacing

\vspace{10mm}
\begin{center}
\large{ \emph{ Maya B. Mathur$^{1, 2\ast}$ and FRIENDS$^{1,3}$ } }
\end{center}

\vspace{20mm}

\small{$^{1}$ Department of Biostatistics, Harvard T. H. Chan School of Public Health, Boston, MA, USA}

\small{$^{2}$Quantitative Sciences Unit, Stanford University, Palo Alto, CA, USA}

\small{$^{3}$Department of Epidemiology, Harvard T. H. Chan School of Public Health, Boston, MA, USA}


\vspace{20mm}
\begin{singlespacing} 
\small{$\ast$: Corresponding author:

mmathur@stanford.edu

Quantitative Sciences Unit (c/o Inna Sayfer)

1070 Arastradero Road

Palo Alto, CA

94305

}
\end{singlespacing}

\vspace{15mm}



\newpage


# Notes to self

Ask Charlie:

\begin{itemize}

\item ML3 results?

\item Share results with Jane Risen?

\item Anything to do before giving final draft to coauthors?

\end{itemize}


Rules:

"Given that we'll have 11 of these, I'd like to keep them as concise as possible. I definitely want to avoid having each paper discuss the motivation for the overall project (e.g., the idea of compare a reviewed protocol to the original one, the lack of full vetting/acceptance of the RP:P protocols, the drawbacks of the one-off RP:P approach relative to this more RRR-like approach, etc.). Those general issues should appear only in the main overview paper so that we don't have 11 papers making the same points. My hope would be to have the finding-specific papers keep the introduction really short, perhaps only a couple of paragraphs. The method and results should be complete enough to convey the study procedures and results fully. Perhaps it would work to refer readers to the OSF project page and the original RP:P materials for details of the original RP:P protocol, with the ML5 papers thoroughly describing the differences between the protocols and the characteristics of the new sample and testing. So, keep the intro and discussions sections for these individual finding paper short (no need for extensive theorizing or overviews) and make the method/results complete enough."

* Say "original" and "updated" protocol (not "endorsed" / "non-endorsed")


# Abstract


# Introduction

\begin{itemize}

\item RPP results

\item ML3 results

\end{itemize}



# Methods

\begin{itemize}

\item{ Design

  \begin{itemize}
  
    \item Endorsed vs. non-endorsed protocol manipulation
    
    \item Details of protocol requirements for in-person sites (prior tasks; solitude requirements)
    
    \item Table 1: Site characteristics, protocol details, and sample sizes (per Google Drive)
  
  \end{itemize}
  
}

\item{ Analysis

  \begin{itemize}
  
    \item Preregistration and sample size determination (mention approx. power)
    
    \item Primary analyses aimed to assess: (1) the target effect (interaction) using only "similar" sites to most closely approximate the author's endorsement requirements; (2) whether the target effect was different in the RPP protocol vs. the similar sites. 
    
    \item Secondary analyses aimed to assess: (1) both of the above but also including all university sites; (2) replicability of main effect; (3) statistical consistency of original with replications; (4) differences in manipulation efficacy between MTurk and universities. 
  
  \end{itemize}
  
}

\end{itemize}



# Results

\begin{itemize}

    \item{ Minimal descriptive stats
  
    \begin{itemize}
      
      \item Fig 1 = 3 interaction plots (MTurk, similar, all universities)
    
    \end{itemize}
  
  }
  
  
  
  \item{Results for target interaction effect
  
    \begin{itemize}
      
      \item No evidence to support target effect in similar sites
      
      \item Same when considering all universities
      
      \item No evidence for differences in target effect between RPP protocol and endorsed protocol
    
    \end{itemize}
  
  }
  
   \item{Results for target interaction effect
  
    \begin{itemize}
      
      \item No evidence to support target effect in similar sites (Fig 2: forest plot for interaction)
      
      \item Directly compare effect size to original
      
      \item Same when considering all universities
      
      \item No evidence for differences in target effect between RPP protocol and endorsed protocol
      
      \item Per post hoc analyses, these results provide some evidence for statistical inconsistency of original with replications. 
    
    \end{itemize}
  
  }
  
    \item{Results for main effect
  
    \begin{itemize}
      
      \item No evidence to support main effect in similar sites (Fig 3: forest plot for main effect)
      
      \item Directly compare effect size to original
      
      \item Same when considering all universities
      
      \item No evidence for differences in between RPP protocol and endorsed protocol
      
      \item Per post hoc analyses, these results weakly suggest statistical inconsistency with original, even though effect size is way smaller. 
    
    \end{itemize}
  
  }
  
\end{itemize}


# Conclusion


\newpage

\doublespacing

\begin{center}
\textbf{ \LARGE{Online Supplement: Analysis Code} }
\vspace{10mm}
\end{center}

\singlespacing


```{r, echo=FALSE, message=FALSE, warnings=FALSE}

# note to self: do not add line breaks to this doc!
#  will give crazy "HaHaHa" error messages, 
#  meaning knitr does not want 2 newlines within a c() or 
#  between function arguments

# load all packages with no annoying warnings
library(knitr)
library(lme4)
library(stargazer)
library(metafor)
library(plyr)
library(ggplot2)
library(rmeta)
library(lmerTest)
library(Replicate)
```

```{r, echo=FALSE}
# turn all code output on/off
opts_chunk$set(echo=TRUE, tidy=FALSE , tidy.opts=list(width.cutoff=60) )
```

\tableofcontents



```{r, echo=FALSE}
# read data from MM's local machine; otherwise use prepped_data.csv file online
setwd("~/Dropbox/Personal computer/Independent studies/Many Labs 5 (ML5)/Linked to OSF/2. Data/Prepped data")
b = read.csv("prepped_data.csv")

# make data dictionary for analysis dataset
# build dictionary from first row of analysis dataset
dict = b[1,]
dict$id = "Subject ID within site; starts at 1 for each site"
dict$site = "Acronym for university or MTurk site"
dict$load = "Indicator for whether subject was under cognitive load (1) or not (0)"
dict$group = "Factor for whether site was MTurk (ref), similar, or dissimilar"
dict$is.mturk = "Indicator for whether site was MTurk (1) or any university, with similar or dissimilar collapsed (0)"
dict$had.read = "Indicator for whether subject imagined having read (1) or not having read the material (0)"
dict$lkl = "Numeric for perceived likelihood"
dict$eff.split = "Numeric for effort split between cognitive load task and reading (as in original study)"
dict$count.eff = "Numeric for amount of effort required for cognitive load task (not in original study; was for possible secondary analyses)"
dict$count.hard = "Numeric for difficulty of cognitive load task (not in original study; was for possible secondary analyses)"
dict$badness = "Numeric for perceived badness of not knowing answer when called on in class (not in original study; was for possible secondary analyses)"
dict$importance = "Numeric for perceived importance of answering correctly in class (not in original study; was for possible secondary analyses)"
dict$end.num = "Numeric for the integer on which subject stopped counting"
dict$excluded = "Indicator for whether subject was excluded from analysis; always = 0 in analysis dataset"
dict$site.n = "Numeric for number of subjects in this site"
dict$site.n.excl = "Numeric for number of subjects excluded in this site"
dict$tempt = "Indicator for whether subject imagined tempting fate (1) or not tempting fate (0); is a reverse-coding of had.read"
dict$site.int.est = "Numeric for interaction estimate within site from OLS (computed by data prep script)"
dict$site.int.lo = "Numeric for interaction estimate CI lower bound within site from OLS (computed by data prep script)"
dict$site.int.hi = "Numeric for interaction estimate CI lower bound within site from OLS (computed by data prep script)"
dict$site.int.SE = "Numeric for SE for interaction estimate within site from OLS (computed by data prep script)"
dict$site.main.est = "Numeric for main effect estimate within site from OLS (computed by data prep script)"
dict$site.main.lo = "Numeric for main effect estimate CI lower bound within site from OLS (computed by data prep script)"
dict$site.main.hi = "Numeric for main effect estimate CI lower bound within site from OLS (computed by data prep script)"
dict$site.main.SE = "Numeric for SE for main effect estimate within site from OLS (computed by data prep script)"
dict$SAT = "Numeric for median SAT score of site (NA if foreign or MTurk)"

setwd("~/Dropbox/Personal computer/Independent studies/Many Labs 5 (ML5)/Linked to OSF/3. Analysis/R code/ML5_risen_gilovich")
write.csv( t( dict[ , -c(1:2) ] ), "analysis_data_dictionary.csv" )
```



# Data Quality 

```{r}
# analysis dataset
setwd("~/Dropbox/Personal computer/Independent studies/Many Labs 5 (ML5)/Linked to OSF/2. Data/Prepped data")
b = read.csv("prepped_data.csv")

# make dataset with only one row per site
first = b[ !duplicated(b$site), ]
# order it by site type, then by largest to smallest n
first = first[ order(first$group, -first$site.n), ]
```

```{r, results='asis'}
# total and excluded bad subjects
d = data.frame( site = first$site, n.excl = first$site.n.excl, n.total = first$site.n)

stargazer(d, header=FALSE, summary=FALSE,
          #column.labels = c("Site", "No. excluded subjects", "No. analyzed subjects"),
          rownames = FALSE,
          title="Excluded and analyzed subjects by site" )
```


```{r, results='asis'}
# sample sizes by site type
t = table( b$group )
stargazer( as.data.frame(t), header=FALSE, summary=FALSE,
           rownames = FALSE,
           colnames = FALSE,
           title = "Total analysis sample sizes by site type" )
```

We excluded subjects exactly per the preregistration and the original study protocol, resulting in `r sum( first$site.n.excl )` exclusions across all sites, including MTurk. This is `r 100 * round( sum( first$site.n.excl ) / sum( first$site.n.excl, first$site.n ), 2 )`\% of the originally collected data.



# Descriptive Stats and Plots

Boxplots: medians and IQRs; lines: simple means by subset. (For the same plots within each site, see the data prep PDF.)  

Note: These aggregated means and SDs pool across all sites within a group (similar, dissimilar, MTurk). We caution that such analyses are potentially subject to bias due to Simpson's Paradox \citep{rucker}, which will be resolved in analysis models below by accounting for clustering by site. They are provided here only as descriptive summaries. The same caveat applies to the following section. 

```{r}
##### Fn: Interaction Plot #####
# pass the desired subset of data
int_plot = function( dat, ggtitle ) {
    agg = ddply( dat, .(load, tempt), summarize, val = mean(lkl, na.rm=TRUE)   )  # aggregate data for plotting happiness
    
  colors = c("black", "orange")
  plot( ggplot( dat, aes(x = as.factor(load), y = lkl, color=as.factor(tempt) ) ) + geom_boxplot(width=0.5) +
      geom_point(data = agg, aes(y = val), size=4 ) +
      geom_line(data = agg, aes(y = val, group = tempt), lwd=2 ) +
      scale_color_manual(values=colors) +
      scale_y_continuous( limits=c(0,10) ) +
      ggtitle(ggtitle) +
      theme_bw()  + xlab("Cognitive load?") + ylab("Perceived likelihood of being called on") +
      guides(color=guide_legend(title="Tempted fate?"))
    )
}

##### Plot By Subset #####
int_plot(b, ggtitle = "All sites (including MTurk)")  # all sites
int_plot(b[ b$group=="b.similar", ], ggtitle = "Similar sites")
int_plot(b[ b$group=="c.dissimilar", ], ggtitle = "Dissimilar sites")
int_plot(b[ b$group=="a.mturk", ], ggtitle = "Mechanical Turk")
```  



## Means and SDs by site type

```{r, results='asis'}
agg.means = aggregate( lkl ~ tempt + load + group, b, mean)
agg.sds = aggregate( lkl ~ tempt + load + group, b, sd)

agg = data.frame( cbind( agg.means, agg.sds$lkl) )
names(agg)[4:5] = c("mean", "SD")

stargazer( agg, header=FALSE, summary=FALSE,
           title = "Means and SDs of perceived likelihood across all subjects
           within each site type (naively pooling all sites)" )
``` 

\newpage

## Forest plots for main effect and interaction

Study-specific estimates are from OLS fit within just that site (this step was completed previously by $\texttt{data\_prep.Rmd}$). Pooled estimates are based on estimated coefficients from LMMs (see preregistered protocol for exact model specification). Throughout, we use "main effect" to refer to the main effect in the condition without cognitive load. 

(Technical note: An alternative for the study-specific estimates would be to use estimates of random intercepts and random slopes by site from the LMM, but here we use subset analyses for a descriptive characterization that relaxes the across-site distributional assumptions of LMM.)

```{r}
# first, fit models that we need for forest plot's pooled estimates
#  and subsequent analyses

# prevent brat forest plot from going off page
opts_chunk$set(echo=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60), fig.width=10 )

# Fn: calculate SE for sum of coefficients
# b1, b2: names of the two coefficients to add
# .mod: the lmer model object
lin_combo = function( b1, b2, .mod ) {
  V = vcov(.mod)
  SE = sqrt( V[b1, b1] + V[b2, b2] + 2 * V[b1, b2] )
  est = fixef(.mod)[b1] + fixef(.mod)[b2]
  lo = as.numeric( est - qnorm(0.975) * SE )
  hi = as.numeric( est + qnorm(0.975) * SE )
  pval = ( 1 - pnorm( abs(est / SE) ) ) * 2
  
  return( data.frame( est, lo, hi, pval ) )
}



##### Only Similar Sites #####
# fit Primary Model 1, to be reported in subsequent section
# reference level for group is MTurk
m1 = lmer( lkl ~ tempt * load * group + (tempt * load | site), data = b[ b$group != "c.dissimilar", ] )

# bizarre mystery: changing order of variables in random slope specification
#  results in convergence failure:
# lmer( lkl ~ tempt * load * group + (load * tempt | site), data = b[ b$group != "c.dissimilar", ] )

# pooled estimate and CI of main effect (similar sites)
main.sim = lin_combo( "tempt", "tempt:groupb.similar", m1 )

# pooled estimate and CI of interaction (similar sites)
int.sim = lin_combo( "tempt:load", "tempt:load:groupb.similar", m1 )



##### Combining All Universities #####
# Model 1' in preregistered protocol
# here, reference level is all university sites
m2 = lmer( lkl ~ tempt * load * is.mturk + (tempt * load | site), data = b )

# pooled estimate and CI of main effect (all universities)
CI2 = confint(m2, method = "Wald")
main.uni = data.frame( est = fixef(m2)["tempt"], lo = CI2[ "tempt", 1 ],
                       hi = CI2[ "tempt", 2 ] )


# pooled estimate and CI of interaction (all universities)
int.uni = data.frame( est = fixef(m2)["tempt:load"], lo = CI2[ "tempt:load", 1 ],
                      hi = CI2[ "tempt:load", 2 ] )

# main effect in MTurk
mturk.main.m2 = lin_combo( "tempt:is.mturk", "tempt", m2 )

# interaction effect in MTurk
mturk.int.m2 = lin_combo( "tempt:load:is.mturk", "tempt:load", m2 )
```



```{r}
# make the forest plot

# Fn: insert spacey elements in vectors for purely cosmetic forest plot reasons
# spaces are between site types
# "use.NA" = should we put NA instead of ""?
pretty_spaces = function(x, use.NA = FALSE){
  x2 = append( x, ifelse( use.NA, NA, "" ), after = 1)
  x2 = append( x2, ifelse( use.NA, NA, "" ), after = 6)
}


# build text "columns" of forest plot
# NAs are for making spaces
tabletext = cbind( c( "Study", "", pretty_spaces( as.character(first$site) ), NA, "Pooled (similar universities)", "Pooled (all universities)" ),
                 c( "Sample size", "", pretty_spaces(first$site.n), NA, sum(first$site.n[first$group=="b.similar"]),
                    sum(first$site.n[ ! first$is.mturk ]) ),
                 c( "Main effect est.", "", pretty_spaces(round( first$site.main.est, 2 )), NA, round( main.sim$est, 2 ),
                    round( main.uni$est, 2 ) )  
                 )

# build columns of point estimates, CI lower, and CI upper values for forest plot
m = c( NA, NA, pretty_spaces(first$site.main.est, use.NA=TRUE), NA, round( main.sim$est, 2 ), round( main.uni$est, 2 ) )
l = c( NA, NA, pretty_spaces(first$site.main.lo, use.NA=TRUE), NA, round( main.sim$lo, 2 ), round( main.uni$lo, 2 ) )
u = c( NA, NA, pretty_spaces(first$site.main.hi, use.NA=TRUE), NA, round( main.sim$hi, 2 ), round( main.uni$hi, 2 ) )
 
forestplot( labeltext=tabletext, mean=m, lower=l, upper=u, 
            zero=0,
            is.summary = c(TRUE, rep(FALSE,14), TRUE, TRUE))
```




```{r}
######## For interaction ########

# build text "columns" of forest plot
# NAs are for making spaces
tabletext = cbind( c( "Study", "", pretty_spaces( as.character(first$site) ), NA, "Pooled (similar universities)", "Pooled (all universities)" ),
                 c( "Sample size", "", pretty_spaces(first$site.n), NA, sum(first$site.n[first$group=="b.similar"]),
                    sum(first$site.n[ ! first$is.mturk ]) ),           
                 c( "Interaction est.", "", pretty_spaces(round( first$site.int.est, 2 )), NA, round( int.sim$est, 2 ),
                    round( int.uni$est, 2 ) )  
                 )

# build columns of point estimates, CI lower, and CI upper values for forest plot
m = c( NA, NA, pretty_spaces(first$site.int.est, use.NA=TRUE), NA, round( int.sim$est, 2 ), round( int.uni$est, 2 ) )
l = c( NA, NA, pretty_spaces(first$site.int.lo, use.NA=TRUE), NA, round( int.sim$lo, 2 ), round( int.uni$lo, 2 ) )
u = c( NA, NA, pretty_spaces(first$site.int.hi, use.NA=TRUE), NA, round( int.sim$hi, 2 ), round( int.uni$hi, 2 ) )
 
forestplot( labeltext=tabletext, mean=m, lower=l, upper=u, 
            zero=0,
            is.summary = c(TRUE, rep(FALSE,14), TRUE, TRUE) )

```

\textbf{Sanity check:} Estimates in the forest plots seem to agree closely with the interaction plots by site type as well as interaction plots for each site ($\texttt{data\_prep.pdf}$).


# Planned Primary Analyses

## Model 1: Observation-level mixed model

Model 1 is a linear mixed model excluding dissimilar sites. We use $X$ to denote tempting fate, $L$ to denote cognitive load, and $Y$ to denote perceived likelihood. 

```{r}
# see section before making forest plot for model fit
# note that reference level for site type is MTurk

# using Wald CIs because profile and boot are struggling to 
#  converge (i.e., assume coefficient estimates are normal,
#  which is quite reasonable at these sample sizes)
CI = confint(m1, method="Wald")

# make table
name = c("Magnitude of X main effect within MTurk", 
          "Magnitude of X main effect within similar sites",
          "Effect of similar site vs. MTurk on X main effect", 
          "Magnitude of X-L interaction within MTurk", 
          "Magnitude of X-L interaction within similar sites",
          "Effect of similar site vs. MTurk on X-L interaction"
          )

value = as.numeric( c( fixef(m1)["tempt"], 
           main.sim$est, 
           fixef(m1)["tempt:groupb.similar"],   
           fixef(m1)["tempt:load"], 
           int.sim$est, 
           fixef(m1)["tempt:load:groupb.similar"]
           ) )
value = round(value, 2)

lo = as.numeric( c( CI["tempt", 1], 
           main.sim$lo, 
           CI["tempt:groupb.similar", 1],
           CI["tempt:load", 1], 
           int.sim$lo, 
           CI["tempt:load:groupb.similar", 1]
           ) )
lo = round(lo, 2)

hi = as.numeric( c( CI["tempt", 2], 
           main.sim$hi, 
           CI["tempt:groupb.similar", 2],
           CI["tempt:load", 2], 
           int.sim$hi, 
           CI["tempt:load:groupb.similar", 2]
           ) )
hi = round(hi, 2)

CI.string = paste( "[", lo, ", ", hi, "]", sep="" )

pvals.m1 = coef(summary(m1))[,5]
pval = as.numeric( c( pvals.m1["tempt"], 
           main.sim$pval, 
           pvals.m1["tempt:groupb.similar"],
           pvals.m1["tempt:load"], 
           int.sim$pval, 
           pvals.m1["tempt:load:groupb.similar"]
           ) )
pval = round(pval, 2)

kable( data.frame( "Name"=name, "Estimate"=value, "CI"=CI.string, "pval"=pval ) )
````

\textbf{Sanity check:} Instead of fitting model that includes both MTurk and similar sites with an interaction of site type, try fitting a model to only the subset of similar sites. 

```{r}
m1.temp = lmer( lkl ~ tempt * load + (tempt * load | site), data = b[ b$group == "b.similar", ] )
CI.temp = confint( m1.temp, method = "Wald" )
```

In the primary model, the estimated main effect was `r round( main.sim$est, 2)` with 95\% CI: (`r round( main.sim$lo, 2)`, `r round( main.sim$hi, 2)`), whereas in the present subset model, it is `r round( fixef(m1.temp)["tempt"], 2)` with 95\% CI: (`r round( CI.temp[row.names(CI.temp) == "tempt", 1], 2)`, `r round( CI.temp[row.names(CI.temp) == "tempt", 2], 2)`).

Also, in the primary model, the estimated interaction effect was `r round( int.sim$est, 2)` with 95\% CI: (`r round( int.sim$lo, 2)`, `r round( int.sim$hi, 2)`), whereas in the present subset model, it is `r round( fixef(m1.temp)["tempt:load"], 2)` with 95\% CI: (`r round( CI.temp[row.names(CI.temp) == "tempt:load", 1], 2)`, `r round( CI.temp[row.names(CI.temp) == "tempt:load", 2], 2)`).

These results are similar.

\newpage

# Planned Secondary Analyses

## Model 1': Observation-level mixed model, including dissimilar sites
We refit the primary analysis model, but now including the dissimilar sites. (This model was actually already fit for the pooled estimate in the forest plots.)


```{r}
# make table
name = c("Magnitude of X main effect within MTurk", 
          "Magnitude of X main effect within university sites",
          "Effect of university site vs. MTurk on X main effect", 
          "Magnitude of X-L interaction within MTurk", 
          "Magnitude of X-L interaction within university sites",
          "Effect of university site vs. MTurk on X-L interaction"
          )

# negative ones are when coefficient is ( MTurk - uni )
value = as.numeric( c( mturk.main.m2$est, 
           fixef(m2)["tempt"], 
           -fixef(m2)["tempt:is.mturk"],  
           mturk.int.m2$est, 
           fixef(m2)["tempt:load"], 
           -fixef(m2)["tempt:load:is.mturk"]
           ) )
value = round(value, 2)

lo = as.numeric( c( mturk.main.m2$lo, 
           CI2[ row.names(CI2) == "tempt", 1 ], 
           -CI2[ row.names(CI2) == "tempt:is.mturk", 1 ],
           mturk.int.m2$lo, 
           CI2[ row.names(CI2) == "tempt:load", 1 ], 
           -CI2[ row.names(CI2) == "tempt:load:is.mturk", 1 ]
           ) )
lo = round(lo, 2)

hi = as.numeric( c( mturk.main.m2$hi, 
           CI2[ row.names(CI2) == "tempt", 2 ], 
           -CI2[ row.names(CI2) == "tempt:is.mturk", 2 ],
           mturk.int.m2$hi, 
           CI2[ row.names(CI2) == "tempt:load", 2 ], 
           -CI2[ row.names(CI2) == "tempt:load:is.mturk", 2 ]
           ) )
hi = round(hi, 2)

CI.string = paste( "[", lo, ", ", hi, "]", sep="" )

pvals.m2 = coef(summary(m2))[,5]
pval = as.numeric( c( mturk.main.m2$pval, 
           pvals.m2["tempt"], 
           pvals.m2["tempt:is.mturk"],
           mturk.int.m2$pval, 
           pvals.m2["tempt:load"], 
           pvals.m2["tempt:load:is.mturk"]
           ) )
pval = round(pval, 2)

kable( data.frame( "Name"=name, "Estimate"=value, "CI"=CI.string, "pval"=pval ) )
````

As a sanity check, work in the "statistical consistency" sections below demonstrates that meta-analytic counterparts to these observation-level models yield nearly identical results. 


## Model 2: Moderation by median SAT score

We treated university sites' median total SAT scores (estimated for 2018) as a proxy for similarity to the site of the original study (Cornell), assuming that universities with higher SAT scores are more similar to Cornell (median SAT: 2134). Universities outside the US and MTurk were given missing values for SAT score. Model 2 assesses whether median SAT score moderates the effect of interest. 

```{r}
# center and scale SAT
b$SATc = ( b$SAT - mean(b$SAT, na.rm=TRUE) ) / sd(b$SAT, na.rm=TRUE)

m.sat = lmer( lkl ~ tempt * load * SATc + (tempt * load | site), data = b )

summary(m.sat)
```

This analysis does not support moderation of either the main effect or the interaction by median SAT score. 

## Refitting original ANOVA model

The original study used two-way ANOVA to test for the main effect and interaction. Per our preregistered protocol, we also reproduce this model as a secondary analysis here. However, we cautio that unlike our primary model, the present analysis that does not account for site is potentially subject to bias due to Simpson's Paradox.

```{r}
summary( aov( lkl ~ load * tempt, data = b[ b$group == "b.similar", ] ) ) 
```

These results are qualitatively similar to what we saw in the primary model. 


## Other planned models

The above analyses did not suggest differences in results between similar and dissimilar sites. Therefore, as planned in the preregistered protocol, we did not pursue the secondary mediation models. 





# Post-Hoc Analyses

## Statistical consistency of main effect estimates between original and replications (similar sites only)


The original study's Experiment 6 reported (for the no-load condition only):

* $\bar{Y}_{X=0, L=0} = 1.90, SD_{Y=0, L=0} = 1.42, n=30$
* $\bar{Y}_{X=1, L=0} = 2.93, SD_{Y=1, L=0} = 2.16, n=30$

```{r}
# effect sizes of original
yi.orig = 2.93 - 1.90
var.mean0 = 1.42^2 / 30
var.mean1 = 2.16^2 / 30
vyi.orig = var.mean0 + var.mean1

# sanity check: try to reproduce t-stat in original paper
yi.orig / sqrt( vyi.orig )
# matches their t= 2.19 (pg 302, column 2)
```

We next estimate the mean and heterogeneity of the site-specific effects among the replications using a mixed model similar to Model 1, except using only similar sites (not MTurk). Note that since these analyses only use the 4 similar sites, heterogeneity estimation is likely to be pretty unstable. 


```{r}
detach("package:lmerTest")
#detach("package:nlme")
m = lmer( lkl ~ tempt * load + (tempt * load | site), data = b[ b$group == "b.similar", ] )
Vhat = 0.05701  # variance of random slopes of tempt
Mhat = fixef(m)[["tempt"]]
SE.Mhat = sqrt(vcov(m)["tempt", "tempt"])
```

Compute $P_{orig}$, i.e., the probability that the original estimate would be as extreme or more extreme than it actually was if drawn from the estimated effect distribution from the replications \citep{mathur_rrr}:

```{r}
( p.orig.main = p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = Mhat, t2 = Vhat, vyr = SE.Mhat^2) )
```

\textbf{Sanity check:} Try meta-analyzing the sites' point estimates instead.

```{r}
meta.main = rma.uni( yi = site.main.est, vi = site.main.SE^2,
                     data=first[ first$group == "b.similar", ],
                     measure="MD", method="PM" )
  
p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = meta.main$b, t2 = meta.main$tau2,
                      vyr = meta.main$vb )
```
It's a bit lower due to the lower estimated heterogeneity here. 

\textbf{Sanity check:} Do these results agree with prediction intervals? They should because there is still basically zero heterogeneity. 


```{r}
pred = pred_int( orig.y = yi.orig, orig.vy = vyi.orig,
                      rep.y = first[first$group=="b.similar",]$site.main.est,
                      rep.vy = first[first$group=="b.similar",]$site.main.SE^2)
```
`r sum( pred$rep.inside )` of 4 similar sites are within their prediction intervals.





## Statistical consistency of interaction estimates between original and replications (similar sites only)


```{r}
# interaction is the "difference in differences"
yi.orig = ( 5.27 - 2.70 ) - ( 2.93 - 1.90 )
vyi.orig = ( 1.42^2 / 30 ) + ( 2.16^2 / 30 ) + ( 2.17^2 / 30 ) + ( 2.36^2 / 30 )
# just add the variances that contribute to the linear combo

# sanity check: reproduce original paper's F-stat
( yi.orig / sqrt( vyi.orig ) )^2  # square a t-stat
# appears within rounding error (reported: F = 4.15)
```

We again estimate the mean and heterogeneity of the site-specific effects among the replications using the same mixed model (among only similar sites) that we fit above. 

```{r}
# same mixed model as above
Vhat = 0.14362  # variance of random slopes of tempt:load
Mhat = fixef(m)[["tempt:load"]]
SE.Mhat = sqrt(vcov(m)["tempt:load", "tempt:load"])
```

Compute $P_{orig}$:

```{r}
p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = Mhat, t2 = Vhat, vyr = SE.Mhat^2)
```

\textbf{Sanity check:} Use meta-analysis instead.

```{r}
meta.int = rma.uni( yi = site.int.est, vi = site.int.SE^2,
                     data=first[ first$group == "b.similar", ],
                     measure="MD", method="PM" )

p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = meta.int$b, t2 = meta.int$tau2,
                      vyr = meta.int$vb )
```


```{r}
pred = pred_int( orig.y = yi.orig, orig.vy = vyi.orig,
                      rep.y = first[first$group=="b.similar",]$site.int.est,
                      rep.vy = first[first$group=="b.similar",]$site.int.SE^2)
```
`r sum( pred$rep.inside )` of 4 similar sites are within their prediction intervals. This seems reasonable given $P_{orig}$.


## Statistical consistency of main effect estimates between original and replications (all university sites)

We now consider consistency of the original study with all university replications. This allows for more precise estimation of heterogeneity. 

```{r}
# effect sizes of original
yi.orig = 2.93 - 1.90
vyi.orig = (1.42^2 + 2.16^2)/2  # within-study variance of the difference
```

Fit a mixed model excluding only MTurk:
```{r}
#detach("package:lmerTest")
#detach("package:nlme")
m = lmer( lkl ~ tempt * load + (tempt * load | site), data = b[ ! b$is.mturk, ] )
Vhat = 0.06692  # variance of random slopes of tempt; manual because extracting the object is huge pain
Mhat = fixef(m)[["tempt"]]
SE.Mhat = sqrt(vcov(m)["tempt", "tempt"])
```

Compute $P_{orig}$:

```{r}
p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = Mhat, t2 = Vhat, vyr = SE.Mhat^2)
```

As a sanity check, try meta-analyzing the sites' point estimates instead:

```{r}
meta.int = rma.uni( yi = site.main.est, vi = site.main.SE^2,
                     data=first[ ! first$is.mturk, ],
                     measure="MD", method="PM" )

p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = meta.main$b, t2 = meta.main$tau2,
                      vyr = meta.main$vb )
```

The estimated main effect and heterogeneity in similar sites was $\widehat{M}=$ `r round( Mhat, 2 )` and $\widehat{V}=$ `r round( Vhat, 2 )` in the mixed model compared to $\widehat{M}=$ `r round( meta.main$b , 2 )` and $\widehat{V}=$ `r round( meta.main$tau2, 2 )` in the meta-analysis. They agree very closely. 


Another sanity check: Do these results, suggestive of good consistency, agree with prediction intervals? They should because there is basically zero heterogeneity. 
```{r}
pred = pred_int( orig.y = yi.orig, orig.vy = vyi.orig,
                      rep.y = first[ ! first$is.mturk, ]$site.main.est,
                      rep.vy = first[ ! first$is.mturk, ]$site.main.SE^2)
```
`r sum( pred$rep.inside )` of 10 university sites are within their prediction intervals. This is close to the 95\% we would expect under consistency.



## Statistical consistency of interaction estimates between original and replications (all university sites)



```{r}
# interaction is the "difference in differences"
yi.orig = ( 5.27 - 2.70 ) - ( 2.93 - 1.90 )
vyi.orig = ( 1.42^2 / 30 ) + ( 2.16^2 / 30 ) + ( 2.17^2 / 30 ) + ( 2.36^2 / 30 )
# just add the variances that contribute to the linear combo
```

We again estimate the mean and heterogeneity of the site-specific effects among the replications using the same mixed model (among only similar sites) that we fit above. 

```{r}
# same mixed model as above
Vhat = 0.05823  # variance of random slopes of tempt:load
Mhat = fixef(m)[["tempt:load"]]
SE.Mhat = sqrt(vcov(m)["tempt:load", "tempt:load"])
```

Compute $P_{orig}$:

```{r}
( p.orig.int = p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = Mhat, t2 = Vhat, vyr = SE.Mhat^2) )
```

As a sensitivity analysis, use meta-analysis instead:

```{r}
meta.int = rma.uni( yi = site.int.est, vi = site.int.SE^2,
                     data=first[ ! first$is.mturk, ],
                     measure="MD", method="PM" )

p_orig( orig.y = yi.orig, orig.vy = vyi.orig,
                      yr = meta.int$b, t2 = meta.int$tau2,
                      vyr = meta.int$vb )
```

The estimated interaction and heterogeneity in similar sites was $\widehat{M}=$ `r round( Mhat, 2 )` and $\widehat{V}=$ `r round( Vhat, 2 )` in the mixed model compared to $\widehat{M}=$ `r round( meta.int$b , 2 )` and $\widehat{V}=$ `r round( meta.int$tau2, 2 )` in the meta-analysis. They agree very closely. 

Sanity check: Do these relatively poor consistency results agree with prediction intervals? 

\textbf{Sanity check:} Do these poor consistency results agree with prediction intervals? 
```{r}
pred = pred_int( orig.y = yi.orig, orig.vy = vyi.orig,
                      rep.y = first[ ! first$is.mturk, ]$site.int.est,
                      rep.vy = first[ ! first$is.mturk, ]$site.int.SE^2)
```
`r sum( pred$rep.inside )` of 10 university sites are within their prediction intervals. This is borderline compared to expectation, as is $P_{orig}$ when compared to the corresponding $\alpha=0.05$ threshold.




## Effectiveness of cognitive load manipulation on MTurk

Is the cognitive load manipulation less effective in MTurk vs. all universities combined? That is, does its effect on the tempt * load interaction vary between MTurk and all universities combined?

```{r, results}
( m = lmer( lkl ~ tempt * load * is.mturk + (tempt * load | site), data = b ) )
# mturk cannot have its own random slope because only one such site
CI = confint(m, method = "Wald")
```

* Effect of MTurk vs. university on effect of cognitive load ($L * Turk$) interaction: `r round( fixef(m)["tempt:load:is.mturk"], 2 )`, 95% CI: `r round( CI[ row.names(CI) == "tempt:load:is.mturk" ], 2 )`. 

Are subjects' reported difficulty or effort associated with the cognitive load manipulation less for MTurk vs. all universities combined?

```{r}
# subset to only subjects actually assigned to cognitive load
# mturk cannot have its own random slope because only one such site
( m3 = lmer( count.eff ~ is.mturk + (1 | site), data = b[ b$load==1, ] ) )
CI3 = confint(m3, method = "Wald")

( m4 = lmer( count.hard ~ is.mturk + (1 | site), data = b[ b$load==1, ] ) )
# cannot include random slopes as random slopes due to convergence problems
CI4 = confint(m4, method = "Wald")
```

* Effect of MTurk vs. university on perceived effort needed for cognitive load task: `r round( fixef(m3)["is.mturk"], 2 )`, 95% CI: `r round( CI3[ row.names(CI3) == "is.mturk" ], 2 )`

* Effect of MTurk vs. university on perceived difficulty of cognitive load task: `r round( fixef(m4)["is.mturk"], 2 )`, 95% CI: `r round( CI4[ row.names(CI4) == "is.mturk" ], 2 )`

\textbf{Summary}: There is no evidence here that the cognitive load manipulation is less effective on MTurk than in universities, either based on its actual effect on likelihood judgements or on its subjective impact.


## More on MTurk vs. college students

How much do students care about answering questions correctly in class by site?
```{r}
kable( aggregate( importance ~ group, FUN = mean, data = b ) )
kable( aggregate( badness ~ group, FUN = mean, data = b ) )

summary( lm( ( b$importance - mean(b$importance, na.rm=TRUE) ) ~ site, data = b ) )
summary( lm( ( b$badness - mean(b$importance, na.rm=TRUE) ) ~ site, data = b ) )
```

Do MTurkers care less than students?
```{r}
summary( lm( ( b$importance - mean(b$importance, na.rm=TRUE) ) ~ is.mturk, data = b ) )
summary( lm( ( b$badness - mean(b$importance, na.rm=TRUE) ) ~ is.mturk, data = b ) )
```
Actually, they care more.


